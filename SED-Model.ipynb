{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":1405119,"sourceType":"datasetVersion","datasetId":821504}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Librabry","metadata":{}},{"cell_type":"code","source":"import collections\nimport collections.abc\ncollections.MutableMapping = collections.abc.MutableMapping\nimport cv2\nimport audioread\nimport logging\nimport os\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport librosa.display as display\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torchsummary import summary\n\nfrom contextlib import contextmanager\nfrom IPython.display import Audio\nfrom pathlib import Path\nfrom typing import Optional, List\n\n# from catalyst.dl import SupervisedRunner, State, CallbackOrder, Callback, CheckpointCallback\nfrom fastprogress import progress_bar\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, average_precision_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:20.975560Z","iopub.execute_input":"2025-05-09T13:37:20.976288Z","iopub.status.idle":"2025-05-09T13:37:26.225620Z","shell.execute_reply.started":"2025-05-09T13:37:20.976258Z","shell.execute_reply":"2025-05-09T13:37:26.225027Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 1.Utils","metadata":{}},{"cell_type":"code","source":"class DFTBase(nn.Module):\n    def __init__(self):\n        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n        super(DFTBase, self).__init__()\n\n    def dft_matrix(self, n):\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n        omega = np.exp(-2 * np.pi * 1j / n)\n        W = np.power(omega, x * y)\n        return W\n\n    def idft_matrix(self, n):\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n        omega = np.exp(2 * np.pi * 1j / n)\n        W = np.power(omega, x * y)\n        return W\n    \n    \nclass STFT(DFTBase):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n        of librosa.core.stft\n        \"\"\"\n        super(STFT, self).__init__()\n\n        assert pad_mode in ['constant', 'reflect']\n\n        self.n_fft = n_fft\n        self.center = center\n        self.pad_mode = pad_mode\n\n        # By default, use the entire frame\n        if win_length is None:\n            win_length = n_fft\n\n        # Set the default hop, if it's not already specified\n        if hop_length is None:\n            hop_length = int(win_length // 4)\n\n        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n\n        # Pad the window out to n_fft size\n        fft_window = librosa.util.pad_center(fft_window, size=n_fft)\n\n        # DFT & IDFT matrix\n        self.W = self.dft_matrix(n_fft)\n\n        out_channels = n_fft // 2 + 1\n\n        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_real.weight.data = torch.Tensor(\n            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft // 2 + 1, 1, n_fft)\n\n        self.conv_imag.weight.data = torch.Tensor(\n            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft // 2 + 1, 1, n_fft)\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, data_length)\n        Returns:\n          real: (batch_size, n_fft // 2 + 1, time_steps)\n          imag: (batch_size, n_fft // 2 + 1, time_steps)\n        \"\"\"\n\n        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n\n        if self.center:\n            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n\n        real = self.conv_real(x)\n        imag = self.conv_imag(x)\n        # (batch_size, n_fft // 2 + 1, time_steps)\n\n        real = real[:, None, :, :].transpose(2, 3)\n        imag = imag[:, None, :, :].transpose(2, 3)\n        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n\n        return real, imag\n    \n    \nclass Spectrogram(nn.Module):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', power=2.0, \n        freeze_parameters=True):\n        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n        Conv1d. The function has the same output of librosa.core.stft\n        \"\"\"\n        super(Spectrogram, self).__init__()\n\n        self.power = power\n\n        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n            win_length=win_length, window=window, center=center, \n            pad_mode=pad_mode, freeze_parameters=True)\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n        Returns:\n          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n        \"\"\"\n\n        (real, imag) = self.stft.forward(input)\n        # (batch_size, n_fft // 2 + 1, time_steps)\n\n        spectrogram = real ** 2 + imag ** 2\n\n        if self.power == 2.0:\n            pass\n        else:\n            spectrogram = spectrogram ** (power / 2.0)\n\n        return spectrogram\n\n    \nclass LogmelFilterBank(nn.Module):\n    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n        the pytorch implementation of as librosa.filters.mel \n        \"\"\"\n        super(LogmelFilterBank, self).__init__()\n\n        self.is_log = is_log\n        self.ref = ref\n        self.amin = amin\n        self.top_db = top_db\n\n        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n            fmin=fmin, fmax=fmax).T\n        # (n_fft // 2 + 1, mel_bins)\n\n        self.melW = nn.Parameter(torch.Tensor(self.melW))\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps)\n        \n        Output: (batch_size, time_steps, mel_bins)\n        \"\"\"\n\n        # Mel spectrogram\n        mel_spectrogram = torch.matmul(input, self.melW)\n\n        # Logmel spectrogram\n        if self.is_log:\n            output = self.power_to_db(mel_spectrogram)\n        else:\n            output = mel_spectrogram\n\n        return output\n\n\n    def power_to_db(self, input):\n        \"\"\"Power to db, this function is the pytorch implementation of \n        librosa.core.power_to_lb\n        \"\"\"\n        ref_value = self.ref\n        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n\n        if self.top_db is not None:\n            if self.top_db < 0:\n                raise ParameterError('top_db must be non-negative')\n            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n\n        return log_spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.226859Z","iopub.execute_input":"2025-05-09T13:37:26.227573Z","iopub.status.idle":"2025-05-09T13:37:26.246846Z","shell.execute_reply.started":"2025-05-09T13:37:26.227545Z","shell.execute_reply":"2025-05-09T13:37:26.246060Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class DropStripes(nn.Module):\n    def __init__(self, dim, drop_width, stripes_num):\n        \"\"\"Drop stripes. \n        Args:\n          dim: int, dimension along which to drop\n          drop_width: int, maximum width of stripes to drop\n          stripes_num: int, how many stripes to drop\n        \"\"\"\n        super(DropStripes, self).__init__()\n\n        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n\n        self.dim = dim\n        self.drop_width = drop_width\n        self.stripes_num = stripes_num\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n\n        assert input.ndimension() == 4\n\n        if self.training is False:\n            return input\n\n        else:\n            batch_size = input.shape[0]\n            total_width = input.shape[self.dim]\n\n            for n in range(batch_size):\n                self.transform_slice(input[n], total_width)\n\n            return input\n\n\n    def transform_slice(self, e, total_width):\n        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n\n        for _ in range(self.stripes_num):\n            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n\n            if self.dim == 2:\n                e[:, bgn : bgn + distance, :] = 0\n            elif self.dim == 3:\n                e[:, :, bgn : bgn + distance] = 0\n\n\nclass SpecAugmentation(nn.Module):\n    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n        freq_stripes_num):\n        \"\"\"Spec augmetation. \n        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n        Args:\n          time_drop_width: int\n          time_stripes_num: int\n          freq_drop_width: int\n          freq_stripes_num: int\n        \"\"\"\n\n        super(SpecAugmentation, self).__init__()\n\n        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n            stripes_num=time_stripes_num)\n\n        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n            stripes_num=freq_stripes_num)\n\n    def forward(self, input):\n        x = self.time_dropper(input)\n        x = self.freq_dropper(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.247640Z","iopub.execute_input":"2025-05-09T13:37:26.248367Z","iopub.status.idle":"2025-05-09T13:37:26.268121Z","shell.execute_reply.started":"2025-05-09T13:37:26.248343Z","shell.execute_reply":"2025-05-09T13:37:26.267478Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 2. Model Architecture","metadata":{}},{"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"Interpolate data in time domain. This is used to compensate the\n    resolution reduction in downsampling of a CNN.\n\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n    is the same as the value of the last frame.\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    pad = framewise_output[:, -1:, :].repeat(\n        1, frames_num - framewise_output.shape[1], 1)\n    \"\"\"tensor for padding\"\"\"\n\n    output = torch.cat((framewise_output, pad), dim=1)\n    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n    return output\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n\n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n\n        return x\n\n\nclass AttBlock(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\",\n                 temperature=1.0):\n        super().__init__()\n\n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.bn_att = nn.BatchNorm1d(out_features)\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.269794Z","iopub.execute_input":"2025-05-09T13:37:26.270084Z","iopub.status.idle":"2025-05-09T13:37:26.290401Z","shell.execute_reply.started":"2025-05-09T13:37:26.270067Z","shell.execute_reply":"2025-05-09T13:37:26.289855Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 2.1 PANNsCNN14Att","metadata":{}},{"cell_type":"code","source":"class PANNsCNN14Att(nn.Module):\n    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n                 mel_bins: int, fmin: int, fmax: int, classes_num: int):\n        super().__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n        self.interpolate_ratio = 32  # Downsampled ratio\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(\n            n_fft=window_size,\n            hop_length=hop_size,\n            win_length=window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(\n            sr=sample_rate,\n            n_fft=window_size,\n            n_mels=mel_bins,\n            fmin=fmin,\n            fmax=fmax,\n            ref=ref,\n            amin=amin,\n            top_db=top_db,\n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(\n            time_drop_width=64,\n            time_stripes_num=2,\n            freq_drop_width=8,\n            freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(mel_bins)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        \n    def cnn_feature_extractor(self, x):\n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        return x\n    \n    def preprocess(self, input, mixup_lambda=None):\n        # t1 = time.time()\n        x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n\n        frames_num = x.shape[2]\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n        return x, frames_num\n        \n\n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n        x, frames_num = self.preprocess(input, mixup_lambda=mixup_lambda)\n\n        # Output shape (batch size, channels, time, frequency)\n        x = self.cnn_feature_extractor(x)\n        \n        # Aggregate in frequency axis\n        x = torch.mean(x, dim=3)\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        # Get framewise output\n        framewise_output = interpolate(segmentwise_output,\n                                       self.interpolate_ratio)\n        framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n        output_dict = {\n            'framewise_output': framewise_output,\n            'clipwise_output': clipwise_output\n        }\n\n        return output_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.291078Z","iopub.execute_input":"2025-05-09T13:37:26.291343Z","iopub.status.idle":"2025-05-09T13:37:26.313956Z","shell.execute_reply.started":"2025-05-09T13:37:26.291326Z","shell.execute_reply":"2025-05-09T13:37:26.313339Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## Config \nclass CFG:\n    seed = 42\n    num_workers = 2\n    OUTPUT_DIR = '/kaggle/working/'\n\n    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    full_train_csv = '/kaggle/input/full-data-2020-2025/all_train_with_paths.csv'\n\n    data2021 = '/kaggle/input/birdclef-2021/train_short_audio/'\n    data2022 = '/kaggle/input/birdclef-2022/train_audio/'\n    data2023 = '/kaggle/input/birdclef-2023/train_audio/'\n    data2024 = '/kaggle/input/birdclef-2024/train_audio/'\n    data2025 = '/kaggle/input/birdclef-2025/train_audio/'\n    data_add = '/kaggle/input/birdclef2024-additional-mp3/additional_audio/'\n\n    model_name = 'tf_efficientnetv2_s.in21k_ft_in1k'\n    pretrained = True\n    in_channels = 1\n\n    FS = 32000\n    TARGET_DURATION = 5\n    TARGET_SHAPE = (256, 256)\n    N_FFT = 1024\n    HOP_LENGTH = 256\n    FMIN = 50\n    FMAX = 14000\n    N_MELS = 256\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    epochs = 10\n    batch_size = 32\n    criterion = 'FocalLossBCE'\n\n    n_fold = 5\n    selected_folds = [0]\n\n    optimizer = 'AdamW'\n    lr = 5e-4\n    weight_decay=1e-5\n\n    scheduler = 'CosineAnnealingLR'\n    min_lr = 1e-6\n    T_max = epochs\n\n    augment_prob = 0.5  \n    mixup_alpha = 0.5\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.314868Z","iopub.execute_input":"2025-05-09T13:37:26.315103Z","iopub.status.idle":"2025-05-09T13:37:26.399018Z","shell.execute_reply.started":"2025-05-09T13:37:26.315087Z","shell.execute_reply":"2025-05-09T13:37:26.398271Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 3. Dataset","metadata":{}},{"cell_type":"code","source":"PERIOD = 5\n\nclass PANNsDataset(data.Dataset):\n    def __init__(self, df, cfg, mode):\n        self.df = df\n        self.cfg = cfg\n        self.mode = mode\n\n        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n        self.species_id = taxonomy_df['primary_label'].tolist()\n        self.label_to_idx = {label: idx for idx, label in enumerate(self.species_id)}\n        self.num_classes = len(self.species_id)\n\n        if 'filepath' not in self.df.columns:\n            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df.filename \n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n\n        y, sr = librosa.load(row['filepath'], sr=32000, mono=True)\n\n        len_y = len(y)\n        effective_length = sr * PERIOD\n        if len_y < effective_length:\n            new_y = np.zeros(effective_length, dtype=y.dtype)\n            start = np.random.randint(effective_length - len_y)\n            new_y[start:start + len_y] = y\n            y = new_y.astype(np.float32)\n        elif len_y > effective_length:\n            start = np.random.randint(len_y - effective_length)\n            y = y[start:start + effective_length].astype(np.float32)\n        else:\n            y = y.astype(np.float32)\n\n        target = self.encode_label(row['primary_label'], row['secondary_labels'])\n        target = torch.tensor(target, dtype=torch.float32)\n\n        return {\"waveform\": y, \"targets\": target}\n\n    def encode_label(self, primary_label, secondary_labels):\n        target = np.zeros(self.num_classes)\n\n        if primary_label in self.label_to_idx:\n            target[self.label_to_idx[primary_label]] = 1.0\n\n        for second_label in secondary_labels:\n            if second_label in self.label_to_idx:\n                target[self.label_to_idx[second_label]] = 1.0\n\n        return target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.399643Z","iopub.execute_input":"2025-05-09T13:37:26.399818Z","iopub.status.idle":"2025-05-09T13:37:26.414186Z","shell.execute_reply.started":"2025-05-09T13:37:26.399803Z","shell.execute_reply":"2025-05-09T13:37:26.413624Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class PANNsLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.bce = nn.BCELoss()\n\n    def forward(self, input, target):\n        input_ = input[\"clipwise_output\"]\n        input_ = torch.where(torch.isnan(input_),\n                             torch.zeros_like(input_),\n                             input_)\n        input_ = torch.where(torch.isinf(input_),\n                             torch.zeros_like(input_),\n                             input_)\n\n        target = target.float()\n\n        return self.bce(input_, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.414844Z","iopub.execute_input":"2025-05-09T13:37:26.415046Z","iopub.status.idle":"2025-05-09T13:37:26.434446Z","shell.execute_reply.started":"2025-05-09T13:37:26.415029Z","shell.execute_reply":"2025-05-09T13:37:26.433791Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# 4. Traning","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.metrics import f1_score, average_precision_score\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport time\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"\n    Huấn luyện mô hình trong một epoch.\n    \n    Args:\n        model: Mô hình PyTorch (PANNsCNN14Att).\n        train_loader: DataLoader cho tập huấn luyện.\n        criterion: Hàm loss (PANNsLoss).\n        optimizer: Optimizer (Adam, SGD, ...).\n        device: Thiết bị (cuda hoặc cpu).\n    \n    Returns:\n        avg_loss: Loss trung bình.\n        avg_f1: F1-score trung bình.\n        avg_ap: Average Precision trung bình.\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_targets = []\n    \n    for batch in tqdm(train_loader, desc=\"Training\"):\n        waveforms = batch[\"waveform\"].to(device)  # (batch_size, data_length)\n        targets = batch[\"targets\"].to(device)     # (batch_size, num_classes)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(waveforms)  # outputs: dict with \"clipwise_output\"\n        loss = criterion(outputs, targets)\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Lưu dự đoán và nhãn để tính metrics\n        preds = outputs[\"clipwise_output\"].detach().cpu().numpy()\n        targets_np = targets.detach().cpu().numpy()\n        all_preds.append(preds)\n        all_targets.append(targets_np)\n    \n    # Tính loss trung bình\n    avg_loss = total_loss / len(train_loader)\n    \n    # Tính metrics\n    # all_preds = np.concatenate(all_preds, axis=0)\n    # all_targets = np.concatenate(all_targets, axis=0)\n    # avg_f1 = f1_score(all_targets, (all_preds > 0.5).astype(int), average=\"micro\")\n    # avg_ap = average_precision_score(all_targets, all_preds, average=\"micro\")\n    \n    # return avg_loss, avg_f1, avg_ap\n\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n    avg_auc = roc_auc_score(all_targets, all_preds, average=\"micro\")\n    \n    return avg_loss, avg_auc\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"\n    Đánh giá mô hình trên tập validation.\n    \n    Args:\n        model: Mô hình PyTorch (PANNsCNN14Att).\n        val_loader: DataLoader cho tập validation.\n        criterion: Hàm loss (PANNsLoss).\n        device: Thiết bị (cuda hoặc cpu).\n    \n    Returns:\n        avg_loss: Loss trung bình.\n        avg_f1: F1-score trung bình.\n        avg_ap: Average Precision trung bình.\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            waveforms = batch[\"waveform\"].to(device)\n            targets = batch[\"targets\"].to(device)\n            \n            # Forward pass\n            outputs = model(waveforms)\n            loss = criterion(outputs, targets)\n            \n            total_loss += loss.item()\n            \n            # Lưu dự đoán và nhãn\n            preds = outputs[\"clipwise_output\"].detach().cpu().numpy()\n            targets_np = targets.detach().cpu().numpy()\n            all_preds.append(preds)\n            all_targets.append(targets_np)\n    \n    # Tính loss trung bình\n    avg_loss = total_loss / len(val_loader)\n\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n    avg_auc = roc_auc_score(all_targets, all_preds, average=\"micro\")\n    \n    return avg_loss, avg_auc\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path=\"best_model.pth\"):\n\n    best_auc = 0.0\n    history = {\"train_loss\": [], \"train_auc\": [],\n               \"val_loss\": [], \"val_auc\": []}\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(\"-\" * 50)\n        \n        train_loss, train_auc = train_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Train Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n        \n        val_loss, val_auc = validate_epoch(model, val_loader, criterion, device)\n        print(f\"Val Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"train_auc\"].append(train_auc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_auc\"].append(val_auc)\n        \n        if val_auc > best_auc:\n            best_auc = val_auc\n            torch.save(model.state_dict(), save_path)\n            print(f\"Saved best model with AUC: {best_auc:.4f}\")\n    \n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.435119Z","iopub.execute_input":"2025-05-09T13:37:26.435365Z","iopub.status.idle":"2025-05-09T13:37:26.454322Z","shell.execute_reply.started":"2025-05-09T13:37:26.435346Z","shell.execute_reply":"2025-05-09T13:37:26.453772Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_config = {\n    \"sample_rate\": 32000,\n    \"window_size\": 1024,\n    \"hop_size\": 320,\n    \"mel_bins\": 64,\n    \"fmin\": 50,\n    \"fmax\": 14000,\n    \"classes_num\": 206\n}\nmodel_config[\"classes_num\"] = 527\nmodel = PANNsCNN14Att(**model_config)\nweights = torch.load(\"/kaggle/input/pannscnn14-decisionlevelatt-weight/Cnn14_DecisionLevelAtt_mAP0.425.pth\")\n# Fixed in V3\nmodel.load_state_dict(weights[\"model\"])\nmodel.att_block = AttBlock(2048, 206, activation='sigmoid')\nmodel.att_block.init_weights()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:26.455964Z","iopub.execute_input":"2025-05-09T13:37:26.456159Z","iopub.status.idle":"2025-05-09T13:37:40.844961Z","shell.execute_reply.started":"2025-05-09T13:37:26.456144Z","shell.execute_reply":"2025-05-09T13:37:40.844295Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2213245902.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weights = torch.load(\"/kaggle/input/pannscnn14-decisionlevelatt-weight/Cnn14_DecisionLevelAtt_mAP0.425.pth\")\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PANNsCNN14Att(\n  (spectrogram_extractor): Spectrogram(\n    (stft): STFT(\n      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n    )\n  )\n  (logmel_extractor): LogmelFilterBank()\n  (spec_augmenter): SpecAugmentation(\n    (time_dropper): DropStripes()\n    (freq_dropper): DropStripes()\n  )\n  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv_block1): ConvBlock(\n    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv_block2): ConvBlock(\n    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv_block3): ConvBlock(\n    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv_block4): ConvBlock(\n    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv_block5): ConvBlock(\n    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv_block6): ConvBlock(\n    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n  (att_block): AttBlock(\n    (att): Conv1d(2048, 206, kernel_size=(1,), stride=(1,))\n    (cla): Conv1d(2048, 206, kernel_size=(1,), stride=(1,))\n    (bn_att): BatchNorm1d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Ví dụ sử dụng\nif __name__ == \"__main__\":\n    # Giả sử bạn đã có các định nghĩa sau từ notebook\n    SR = 32000\n    model_config = {\n        \"sample_rate\": SR,\n        \"window_size\": 1024,\n        \"hop_size\": 320,\n        \"mel_bins\": 64,\n        \"fmin\": 50,\n        \"fmax\": 14000,\n        \"classes_num\": 206\n    }\n    \n    criterion = PANNsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    df = pd.read_csv('/kaggle/input/birdclef-2025/train.csv')\n    \n    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n        if fold not in cfg.selected_folds:\n            continue\n        else:\n            train_df = df.iloc[train_idx].reset_index(drop=True)\n            val_df = df.iloc[val_idx].reset_index(drop=True)\n    \n    train_dataset = PANNsDataset(train_df, cfg, 'train')\n    val_dataset = PANNsDataset(val_df, cfg, 'val')\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n    \n    # Huấn luyện mô hình\n    history = train_model(\n        model,\n        train_loader,\n        val_loader,\n        criterion,\n        optimizer,\n        num_epochs=10,\n        device=device,\n        save_path=\"panns_best_model.pth\"\n    )\n    \n    print(\"\\nTraining History:\")\n    for epoch in range(len(history[\"train_loss\"])):\n        print(f\"Epoch {epoch + 1}:\")\n        print(f\"  Train - Loss: {history['train_loss'][epoch]:.4f}, Train - AUC: {history['train_auc'][epoch]:.4f}\")\n        print(f\"  Val   - Loss: {history['val_loss'][epoch]:.4f}, Val - AUC: {history['val_auc'][epoch]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:37:40.845748Z","iopub.execute_input":"2025-05-09T13:37:40.846096Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|█████▍    | 777/1429 [04:07<02:49,  3.85it/s]","output_type":"stream"}],"execution_count":null}]}